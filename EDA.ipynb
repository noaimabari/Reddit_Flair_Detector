{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We start by first loading our first dataset which contains the following information:\n",
    "\n",
    "1. title: Title of the reddit post\n",
    "2. created: Time at which the post was created\n",
    "3. author : Name of the post's author\n",
    "4. No. of comments : Number of comments on the post\n",
    "5. url : URL of the reddit post\n",
    "6. body : Body of the post if any\n",
    "7. score: The number of upvotes minus the number of downvotes on the post\n",
    "8. id: ID of the post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data3.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From a quick look at the raw data , we can make the following assumptions: \n",
    "\n",
    "1. The body column contains mostly nan values. \n",
    "2. The id column holds no specific significance both in classification of the post or in extracting any meaningful insights from the post. \n",
    "3. The created column contains only the time at which the post was created, which might not give any significant insights into the data. \n",
    "\n",
    "It is hence best to drop out these columns before we perform exploratory data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing unwanted columns\n",
    "\n",
    "data.drop(['body', 'id', 'created'], axis = 1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "In this section I am adding certain additional features to my dataset, so as to study the text data in the title column better\n",
    "\n",
    "1. title_len = Length of the title\n",
    "2. word_count = Total no. of words in the title\n",
    "3. polarity = Sentiment polarity of the given title on a scale of [-1 to 1] where -1 signifies most -ve and +1 most +ve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## adding extra features\n",
    "\n",
    "data['title_len'] = data['title'].astype(str).apply(len)\n",
    "data['word_count'] = data['title'].apply(lambda x: len(str(x).split()))\n",
    "data[\"polarity\"] = data[\"title\"].map(lambda text: TextBlob(text).sentiment.polarity)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Visualization\n",
    "\n",
    "This step is performed in order to get the summary statistics for each field in the data set:\n",
    "1. Sentiment Polarity Distribution\n",
    "2. Title Word Count Distribution\n",
    "3. Title Length Distribution\n",
    "4. Mean scores for different flairs\n",
    "5. Total number of comments in different flairs\n",
    "\n",
    "It helps us to understand the dataset better, and gives us insight on what could be our approach towards cleaning of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Polarity Distribution in Title text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data[\"polarity\"], bins = 10, edgecolor = 'black', color = 'purple', alpha = 0.5)\n",
    "plt.title('Sentiment Polarity Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above histogram, it can be seen that most of the text data in title column is of neutral sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('5 random posts with the highest positive sentiment polarity: \\n')\n",
    "cl = data.loc[data.polarity == 1, ['title']].sample(5).values\n",
    "for c in cl:\n",
    "    print(c[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('5 random posts with the highest neutral sentiment polarity: \\n')\n",
    "cl = data.loc[data.polarity == 0, ['title']].sample(5).values\n",
    "for c in cl:\n",
    "    print(c[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('5 random posts with the highest negative sentiment polarity: \\n')\n",
    "cl = data.loc[data.polarity == -1, ['title']].sample(5).values\n",
    "for c in cl:\n",
    "    print(c[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word count distribution of title text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data[\"word_count\"], bins = 10, edgecolor = 'black', color = 'blue', alpha = 0.5)\n",
    "plt.title('Word Count Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Title Length Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data[\"title_len\"], bins = 10, edgecolor = 'black', color = 'red', alpha = 0.5)\n",
    "plt.title('Title Length Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The title length distribution shows that the title text is relatively smaller in terms of the number of characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Scores of Different Flairs:\n",
    "\n",
    "A submission's score in reddit is simply the number of upvotes minus the number of downvotes. By studying the mean scores for different flairs, we get to see how popular different reddit flairs are among the users. It might tell us if we can also use scores as a distinguishing feature in flairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finding the mean scores\n",
    "\n",
    "scores = {}\n",
    "for f in data[\"flair\"].unique():\n",
    "    scores[f] = data[data[\"flair\"]==f].describe()[\"score\"].mean()\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting the mean scores\n",
    "\n",
    "plt.bar(x = list(scores.keys()), height = list(scores.values()), color = 'magenta', edgecolor = 'black', alpha = 0.5)\n",
    "plt.xticks(rotation=80)\n",
    "plt.xlabel(\"Flair\")\n",
    "plt.ylabel(\"Mean Score\")\n",
    "plt.title(\"Mean score of different flairs\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of comments in different flairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flairs = list(data.groupby(['flair'])['No. of comments'].sum().index)\n",
    "sum_of_no_of_comments = list(data.groupby(['flair'])['No. of comments'].sum().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pie(sum_of_no_of_comments,labels = flairs, explode = [0.15, 0.15 ,0.15 ,0.15, 0.15, 0.15 ,0.15, 0.15, 0.15, 0.15, 0.15], autopct=\"%.1f%%\")\n",
    "plt.title(\"Total Number of comments for different flairs\\n\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequently occuring words in title text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_words(corpus, n=None):\n",
    "    vec = CountVectorizer().fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "common_words = get_top_n_words(data['title'], 20)\n",
    "df1 = pd.DataFrame(common_words, columns = ['title' , 'count'])\n",
    "words = list(df1.groupby('title').sum()['count'].sort_values(ascending=False).index)\n",
    "count = list(df1.groupby('title').sum()['count'].sort_values(ascending=False).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(words, count, color = 'blue', edgecolor = 'black', alpha = 0.5)\n",
    "plt.title(\"Top 20 words in title text before removing stop words\")\n",
    "plt.xticks(rotation = 80)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_words_stop(corpus, n=None):\n",
    "    vec = CountVectorizer(stop_words = 'english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "common_words = get_top_n_words_stop(data['title'], 20)\n",
    "df2 = pd.DataFrame(common_words, columns = ['title' , 'count'])\n",
    "df2.groupby('title').sum()['count'].sort_values(ascending=False)\n",
    "words = list(df2.groupby('title').sum()['count'].sort_values(ascending=False).index)\n",
    "count = list(df2.groupby('title').sum()['count'].sort_values(ascending=False).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(words, count, color = 'red', edgecolor = 'black', alpha = 0.5)\n",
    "plt.title(\"Top 20 words in title text after removing stop words\")\n",
    "plt.xticks(rotation = 80)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The above visualization stresses on the importance of removing stopwords from our dataset as they are present in all categories of text and hold little importance when it comes to classification\n",
    "#### After removal of stopwords, we find that few words are present in large numbers, such as coronavirus, which is present  in almost all of the categories. This may pose a problem when it comes to classification as due to the ongoing covid19 crisis, the content of a lot of flairs are similar and revolve around coronavirus. This may confuse our classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now exploring comments data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA on comments data\n",
    "\n",
    "Since the comments data is large and contains a lot of impurity in terms of symbols and characters, we need to preprocess the data a little so that we can get better insights in the actual content of data.\n",
    "\n",
    "1. We first start by loading the data from a csv file to a pandas dataframe\n",
    "2. Next we preprocess the comments data and remove the bad symbols\n",
    "3. Since the body of comments includes various top comments, we separate sentences to form a new dataframe\n",
    "4. Next we perform feature engineering on our data, ie. adding additional features such as comment lenghts, sentiment polarity and word count.\n",
    "5. After this, we perform univariate visualization on the data \n",
    "6. A visualisation of the top words before and after removing stopwords is also performed on the given data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Loading data\n",
    "comment_data = pd.read_csv(\"data4.csv\")\n",
    "comment_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_data['body'][99]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From a look at the raw data above, we can see that the comment data is highly impure and contains a lot of bad symbols, poor formatting, hindi/hinglish words which may make it difficult for us both for performing EDA and later in classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preproccesing the data before EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing bad words\n",
    "\n",
    "def preprocess(comments):\n",
    "    comments = comments.str.replace(\"//\", \"\")\n",
    "    comments = comments.str.replace('[',  \"\")\n",
    "    comments = comments.str.replace('=',\"\")\n",
    "    comments = comments.str.replace(']',\"\")\n",
    "    comments = comments.str.replace('[',\"\")\n",
    "    comments = comments.str.replace(')', '')\n",
    "    comments = comments.str.replace('(', '')\n",
    "    comments = comments.str.replace('\\\\n', '')\n",
    "    comments = comments.str.replace('\\\\t', '')\n",
    "    comments = comments.str.replace('\\\\', '')\n",
    "    comments = comments.str.replace('@', '')  \n",
    "    comments = comments.str.replace('<', '') \n",
    "    comments = comments.str.replace('>', '') \n",
    "\n",
    "    return comments\n",
    "comment_data[\"body\"] = preprocess(comment_data['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import re\n",
    "#comment_data['body'] = comment_data['body'].apply(lambda comment : re.sub(r'[\\xe2\\x80\\x99s]', '', str(comment)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## breaking the data into separate sentences along with their respective flairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flairs = np.array(comment_data[\"flair\"])\n",
    "comment_text = np.array(comment_data[\"body\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = []\n",
    "c = []\n",
    "for i in range(len(comment_text)):\n",
    "    comments_list = sent_tokenize(str(comment_text[i]))\n",
    "    for comment in comments_list:\n",
    "        f.append(flairs[i])\n",
    "        c.append(comment.lower()) \n",
    "f = np.array(f)\n",
    "c = np.array(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.shape, f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = []\n",
    "for i in range(len(f)):\n",
    "    d.append((f[i],c[i]))\n",
    "d[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_data = pd.DataFrame(d, columns = [\"flair\", \"body\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing NaN values from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_data = comment_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## adding extra features\n",
    "comment_data['comments_len'] = comment_data['body'].astype(str).apply(len)\n",
    "comment_data['word_count'] = comment_data['body'].apply(lambda x: len(str(x).split()))\n",
    "comment_data[\"polarity\"] = comment_data[\"body\"].map(lambda text: TextBlob(str(text)).sentiment.polarity)\n",
    "comment_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(comment_data[\"polarity\"], bins = 10, edgecolor = 'black', color = 'purple', alpha = 0.5)\n",
    "plt.title('Sentiment Polarity Distribution(Comments)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_data['word_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(comment_data[\"word_count\"], bins = 50, edgecolor = 'black', color = 'blue', alpha = 0.5)\n",
    "plt.title('Word Count Distribution (Comments)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(comment_data[\"comments_len\"], bins = 50, edgecolor = 'black', color = 'red', alpha = 0.5)\n",
    "plt.title('Comment Length Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = get_top_n_words(comment_data['body'], 20)\n",
    "df1 = pd.DataFrame(common_words, columns = ['comment' , 'count'])\n",
    "words = list(df1.groupby('comment').sum()['count'].sort_values(ascending=False).index)\n",
    "count = list(df1.groupby('comment').sum()['count'].sort_values(ascending=False).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(words, count, color = 'navy', edgecolor = 'black', alpha = 0.5)\n",
    "plt.title(\"Top 20 words in title text before removing stop words\")\n",
    "plt.xticks(rotation = 80)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = get_top_n_words_stop(comment_data['body'], 20)\n",
    "df2 = pd.DataFrame(common_words, columns = ['comment' , 'count'])\n",
    "df2.groupby('comment').sum()['count'].sort_values(ascending=False)\n",
    "words = list(df2.groupby('comment').sum()['count'].sort_values(ascending=False).index)\n",
    "count = list(df2.groupby('comment').sum()['count'].sort_values(ascending=False).values)\n",
    "plt.bar(words, count, color = 'violet', edgecolor = 'black', alpha = 0.5)\n",
    "plt.title(\"Top 20 words in title text after removing stop words\")\n",
    "plt.xticks(rotation = 80)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('5 random posts with the highest positive sentiment polarity: \\n')\n",
    "cl = comment_data.loc[comment_data.polarity == 1, ['body']].sample(5).values\n",
    "for c in cl:\n",
    "    print(c[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('5 random posts with the highest positive sentiment polarity: \\n')\n",
    "cl = comment_data.loc[comment_data.polarity == 0, ['body']].sample(5).values\n",
    "for c in cl:\n",
    "    print(c[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('5 random posts with the highest positive sentiment polarity: \\n')\n",
    "cl = comment_data.loc[comment_data.polarity == -1, ['body']].sample(5).values\n",
    "for c in cl:\n",
    "    print(c[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One of the biggest issue with comments data is that comments in r/India posts are mostly in hindi, hinglish or poor english.\n",
    "#### There are also a lot of emojis, abuse words, etc which the classifier may not understand."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
