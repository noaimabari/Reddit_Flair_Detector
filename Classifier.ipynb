{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing all necessary modules\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading both the datasets obtained previously\n",
    "\n",
    "data = pd.read_csv('data3.csv')\n",
    "comment_data = pd.read_csv('data4.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions for preprocessing and cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating a list of stopwords and punctuations\n",
    "\n",
    "stop = stopwords.words(\"english\")\n",
    "punctuations = list(string.punctuation)\n",
    "stop = stop + punctuations ## to remove punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing bad symbols\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.str.replace(\"//\", \" \")\n",
    "    text = text.str.replace(\"/\", \" \")\n",
    "    text = text.str.replace(\":\", \" \")\n",
    "    text = text.str.replace(\"'\\'\", \" \")\n",
    "    text = text.str.replace(\"'\", \" \")\n",
    "    text = text.str.replace('[',  \" \")\n",
    "    text = text.str.replace('=',\" \")\n",
    "    text = text.str.replace(']',\" \")\n",
    "    text = text.str.replace('[',\" \")\n",
    "    text = text.str.replace(')', ' ')\n",
    "    text = text.str.replace('(', ' ')\n",
    "    text = text.str.replace('\\\\n', ' ')\n",
    "    text = text.str.replace('\\\\t', ' ')\n",
    "    text = text.str.replace('\\\\', ' ')\n",
    "    text = text.str.replace('@', ' ')  \n",
    "    text = text.str.replace('<', ' ') \n",
    "    text = text.str.replace('>', ' ') \n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating an object for lemmatization\n",
    "\n",
    "lt = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fucntion to get pos_tag of a word\n",
    "\n",
    "def get_simple_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to remove emoji's from the text\n",
    "\n",
    "def deEmojify(inputString):\n",
    "    return inputString.encode('ascii', 'ignore').decode('ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## combining the above three functions \n",
    "## cleaning words\n",
    "## removing stop words, punctuations, emojis\n",
    "## performing lemmatization on the words\n",
    "\n",
    "def clean_review(words):\n",
    "    output_words = []\n",
    "    for w in words:\n",
    "        w = deEmojify(w) # removing emojis\n",
    "        if w.lower() not in stop: # removing stopwords\n",
    "            try:\n",
    "                pos = pos_tag([w]) # finding the pos tag of the word\n",
    "                clean_word = lt.lemmatize(w,pos = get_simple_pos(pos[0][1])) # lemmatizing the word\n",
    "                output_words.append(clean_word.lower()) # adding the word in lower case as cleaned word\n",
    "            except:\n",
    "                continue\n",
    "    return np.array(output_words) # returning the array of cleaned words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USING TITLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2552,), (2552,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## loading the text and labels y from our dataframe\n",
    "\n",
    "text = np.array(data[\"title\"])\n",
    "y = np.array(data[\"flair\"])\n",
    "text.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = text\n",
    "y_train = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1914,), (1914,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## splitting the data into 2 parts, one for training and one for testing the performance of the classifier\n",
    "\n",
    "train_text, test_text, y_train, y_test = train_test_split(text, y, random_state = 1)\n",
    "train_text.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Some', 'mind', 'blowing', 'facts', 'about', 'Manmohan', 'Singh'],\n",
       " 'Science/Technology')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## forming training documents with each entry as a tuple containing text and the respective flair\n",
    "## ie. documents = [('jhd odjsd jfs .. ', 'AskIndia'), ('dsjd...', 'Politics') ..]\n",
    "\n",
    "documents = [] ## training documents\n",
    "for i in range(len(y_train)):\n",
    "    documents.append((train_text[i].split(\" \"),y_train[i]))\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cleaning the training data, removing stopwords, punctuations, emojis, and lemmatizing the words\n",
    "\n",
    "documents = [(clean_review(i),category) for i,category in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['mind', 'blowing', 'fact', 'manmohan', 'singh'], dtype='<U8'),\n",
       " 'Science/Technology')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0] # a look at how the first entry looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now forming x_train and y_train from the cleaned data(ie documents)\n",
    "\n",
    "y_train = [category for words,category in documents]\n",
    "x_train = [\" \".join(words) for words, categories in documents] # joining the cleaned words to form text for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## forming sparse matrix of x_train and x_test using TfidfVectorizer which can be directly used as input in sklearn classifiers\n",
    "\n",
    "token_vec = TfidfVectorizer(max_features = 3000, ngram_range = (1,3))\n",
    "x_train = token_vec.fit_transform(x_train) ## we can use the sparse matrix directyly as training and testing data in our sklearn classifiers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## transforming x_test into a sparse matrix\n",
    "\n",
    "x_test = token_vec.transform(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data is now ready for being used in sklearn classifiers\n",
    "## x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification using different classifiers:\n",
    "\n",
    "1. Multinomial Naive Bayes\n",
    "2. Logistic Regression\n",
    "3. Random Forests\n",
    "4. Linear SVM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=2, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1 = MultinomialNB(alpha = 2)\n",
    "clf1.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6520376175548589"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.5, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2 = LogisticRegression(C = 0.5)\n",
    "clf2.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6959247648902821"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf3 = RandomForestClassifier() \n",
    "clf3.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6865203761755486"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf3.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf4 = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf4 = SVC()\n",
    "clf4.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6551724137931034"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf4.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## I have combined all the above steps till obtaining the sparse matrices for our data into a single function get_data\n",
    "## in order to avoid redundancy of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to return the data in the required form as sparse martices to be used in sklearn classifiers\n",
    "def get_data(data, key):\n",
    "    \n",
    "    #data[key] = preprocess(data[key]) ## preprocessing the data by removing bad symbols etc\n",
    "    #print(data[key])\n",
    "    #data.dropna(inplace = True) ## removing rows having nan values from the dataframe\n",
    "    text = np.array(data[key])\n",
    "    y = np.array(data['flair'])\n",
    "    train_text, test_text, y_train, y_test = train_test_split(text, y, random_state = 1) ## splitting the data into train and test\n",
    "    documents = [] ## forming training documents with each entry as a tuple containing text and the respective flair\n",
    "    ## ie. documents = [('jhd odjsd jfs .. ', 'AskIndia'), ('dsjd...', 'Politics') ..]\n",
    "    for i in range(len(y_train)):\n",
    "        documents.append((train_text[i].split(\" \"),y_train[i]))\n",
    "    ## cleaning the training documents \n",
    "    documents = [(clean_review(i),category) for i,category in documents] ## clean review function defined above\n",
    "    print(documents[0])\n",
    "    ## now forming x_train and y_train from the cleaned data(ie documents)\n",
    "    y_train = [category for words,category in documents]\n",
    "    x_train = [\" \".join(words) for words, categories in documents] ## joining the cleaned words of the documents\n",
    "    ## forming sparse matrix of x_train and x_test using TfidfVectorizer\n",
    "    ## creating an object of TfidfVectorizer with 3000 words as features and an ngram range of (1,3)\n",
    "    token_vec = TfidfVectorizer(max_features = 3000, ngram_range = (1,3))\n",
    "    x_train = token_vec.fit_transform(x_train) \n",
    "    x_test = token_vec.transform(test_text)\n",
    "    return x_train, x_test, y_train, y_test, token_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using URL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## New preprocessing function for removing bad symbols from a url\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.str.replace(\"//\", \" \")\n",
    "    text = text.str.replace('.', ' ')\n",
    "    text = text.str.replace('https:', ' ')\n",
    "    text = text.str.replace('_', ' ')\n",
    "    text = text.str.replace('-', ' ')\n",
    "    text = text.str.replace(\"/\", \" \")\n",
    "    text = text.str.replace(\"'\\'\", \" \")\n",
    "    text = text.str.replace(\"'\", \" \")\n",
    "    text = text.str.replace('[',  \" \")\n",
    "    text = text.str.replace('=',\" \")\n",
    "    text = text.str.replace(']',\" \")\n",
    "    text = text.str.replace('[',\" \")\n",
    "    text = text.str.replace(')', ' ')\n",
    "    text = text.str.replace('(', ' ')\n",
    "    text = text.str.replace('\\\\n', ' ')\n",
    "    text = text.str.replace('\\\\t', ' ')\n",
    "    text = text.str.replace('\\\\', ' ')\n",
    "    text = text.str.replace('@', ' ')  \n",
    "    text = text.str.replace('<', ' ') \n",
    "    text = text.str.replace('>', ' ') \n",
    "    text = text.str.replace(\"'\", ' ') \n",
    "\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['url'] = preprocess(data['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array(['www', 'reddit', 'com', 'r', 'india', 'comment', 'd9fh9n', 'mind',\n",
      "       'blowing', 'fact', 'manmohan', 'singh'], dtype='<U8'), 'Science/Technology')\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test, token_vec = get_data(data, 'url')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  www telegraphindia com india coronavirus outbreak untouchability even in quarantine cid 1764186'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['url'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=2, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1 = MultinomialNB(alpha = 2)\n",
    "clf1.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4169278996865204"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2 = LogisticRegression()\n",
    "clf2.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47648902821316613"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf3 = RandomForestClassifier()\n",
    "clf3.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4670846394984326"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf3.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf4 = SVC()\n",
    "clf4.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4608150470219436"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf4.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USING COMMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flair</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Scheduled</td>\n",
       "      <td>Let them feel hungry for a couple of days, max...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Scheduled</td>\n",
       "      <td>This is beyond petty.&gt; The inclusion of a Delh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Scheduled</td>\n",
       "      <td>My hunch is , this guy is trying to expose the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Scheduled</td>\n",
       "      <td>Muslims and reservation are two distractions u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Scheduled</td>\n",
       "      <td>Bachega India, tabhi toh Padhega India.Gand ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2547</th>\n",
       "      <td>Coronavirus</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2548</th>\n",
       "      <td>Coronavirus</td>\n",
       "      <td>They did a FAB job! Hats off to the whole team.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2549</th>\n",
       "      <td>Coronavirus</td>\n",
       "      <td>Currently seeing the numbers in Mumbai i don't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2550</th>\n",
       "      <td>Coronavirus</td>\n",
       "      <td>Yeah u need a skullcap to be virus here /sHis ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2551</th>\n",
       "      <td>Coronavirus</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2552 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            flair                                               body\n",
       "0       Scheduled  Let them feel hungry for a couple of days, max...\n",
       "1       Scheduled  This is beyond petty.> The inclusion of a Delh...\n",
       "2       Scheduled  My hunch is , this guy is trying to expose the...\n",
       "3       Scheduled  Muslims and reservation are two distractions u...\n",
       "4       Scheduled  Bachega India, tabhi toh Padhega India.Gand ma...\n",
       "...           ...                                                ...\n",
       "2547  Coronavirus                                                NaN\n",
       "2548  Coronavirus    They did a FAB job! Hats off to the whole team.\n",
       "2549  Coronavirus  Currently seeing the numbers in Mumbai i don't...\n",
       "2550  Coronavirus  Yeah u need a skullcap to be virus here /sHis ...\n",
       "2551  Coronavirus                                                NaN\n",
       "\n",
       "[2552 rows x 2 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_data = pd.read_csv('data4.csv')\n",
    "comment_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flair</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Scheduled</td>\n",
       "      <td>Let them feel hungry for a couple of days, max...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Scheduled</td>\n",
       "      <td>This is beyond petty.&gt; The inclusion of a Delh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Scheduled</td>\n",
       "      <td>My hunch is , this guy is trying to expose the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Scheduled</td>\n",
       "      <td>Muslims and reservation are two distractions u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Scheduled</td>\n",
       "      <td>Bachega India, tabhi toh Padhega India.Gand ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2544</th>\n",
       "      <td>Coronavirus</td>\n",
       "      <td>&gt; Kabootar or Kachori, no one will be spared. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2545</th>\n",
       "      <td>Coronavirus</td>\n",
       "      <td>Most of those isolation wards are dangerous wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2548</th>\n",
       "      <td>Coronavirus</td>\n",
       "      <td>They did a FAB job! Hats off to the whole team.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2549</th>\n",
       "      <td>Coronavirus</td>\n",
       "      <td>Currently seeing the numbers in Mumbai i don't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2550</th>\n",
       "      <td>Coronavirus</td>\n",
       "      <td>Yeah u need a skullcap to be virus here /sHis ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2295 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            flair                                               body\n",
       "0       Scheduled  Let them feel hungry for a couple of days, max...\n",
       "1       Scheduled  This is beyond petty.> The inclusion of a Delh...\n",
       "2       Scheduled  My hunch is , this guy is trying to expose the...\n",
       "3       Scheduled  Muslims and reservation are two distractions u...\n",
       "4       Scheduled  Bachega India, tabhi toh Padhega India.Gand ma...\n",
       "...           ...                                                ...\n",
       "2544  Coronavirus  > Kabootar or Kachori, no one will be spared. ...\n",
       "2545  Coronavirus  Most of those isolation wards are dangerous wh...\n",
       "2548  Coronavirus    They did a FAB job! Hats off to the whole team.\n",
       "2549  Coronavirus  Currently seeing the numbers in Mumbai i don't...\n",
       "2550  Coronavirus  Yeah u need a skullcap to be virus here /sHis ...\n",
       "\n",
       "[2295 rows x 2 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_data.dropna(inplace = True)\n",
    "comment_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_data['body'] = preprocess(comment_data['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array(['time', 'ritual', 'smellwaiting', 'next', 'task', 'declare',\n",
      "       'whatever', 'want', 'declare', '1', '2', 'day', 'before?', 'last',\n",
      "       'minute', 'declaration', 'cause', 'chaos', 'hope', 'red', 'orange',\n",
      "       'green', 'thing', '10am', '8pm', 'might', 'relax', 'condition',\n",
      "       'make', 'life', 'easy', 'non', 'impact', 'region', 'ramayan', 'ke',\n",
      "       'baad', 'siddha', 'modiji', 'ke', 'darshanany', 'guess', 'would',\n",
      "       'new', 'task', '?tn', 'already', 'extend', 'lockdown', 'till',\n",
      "       'april', '30', 'tweet', 'come', 'tweet', 'twitter', 'com',\n",
      "       'pmoindia', 'status', '1249620775679610880?s', '21', 'bet',\n",
      "       'would', 'full', 'lockdown', 'extend', 'behalf', 'shift',\n",
      "       'decision', 'cm', 'respectively', 'state', 'bjp', 'shame', 'later',\n",
      "       'oppose', 'party', 'state', 'come', 'directly', 'tv,', 'first',\n",
      "       'announcement', 'give', 'time', 'everyone', 'glue', 'tv', 'poora',\n",
      "       'tamasha', 'banaa', 'diya', 'hai', 'fir', 'shuru', 'tamasha',\n",
      "       'popcorn', 'timecan', 'fuck', 'press', 'releasei', 'urge',\n",
      "       'people', 'drink', 'gomutra', '15:00', '15', 'th', 'april',\n",
      "       'support', 'people', 'work', 'keep', 'u', 'karuna', 'pm', 'sstock',\n",
      "       'gomutra', 'citizenswhy', 'press', 'conference', 'like', 'us?',\n",
      "       'everytime', 'video', 'releasedbhai', 'daru', 'kohl', 'dena',\n",
      "       'delete'], dtype='<U21'), 'Coronavirus')\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test, token_vec = get_data(comment_data, 'body')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=2, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1 = MultinomialNB(alpha = 2)\n",
    "clf1.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30139372822299654"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = LogisticRegression(tol = 0.00001, max_iter = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=1e-05, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4146341463414634"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf3 = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf3.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44076655052264807"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf3.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf4 = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf4.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31881533101045295"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf4.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TITLE + URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading both the datasets\n",
    "\n",
    "data1 = pd.read_csv('data3.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['url'] = preprocess(data1['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "## forming a new dataframe containing both title of the reddit post along with the text of top comments\n",
    "\n",
    "df = pd.DataFrame(data1['title'] + data1['url'], columns = ['title + url'])\n",
    "df['flair'] = data1['flair']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title + url</th>\n",
       "      <th>flair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Untouchability, even in quarantine. 'We have n...</td>\n",
       "      <td>Scheduled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Delhi Govt Sources: Names of CM Arvind Kejriwa...</td>\n",
       "      <td>Scheduled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Delhi: AP Singh, advocate of 2012 Delhi gang-r...</td>\n",
       "      <td>Scheduled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why the Supreme Court’s verdict on SC/ST quota...</td>\n",
       "      <td>Scheduled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What about the entrance exams scheduled in May...</td>\n",
       "      <td>Scheduled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2547</th>\n",
       "      <td>Coronavirus: The race to stop the virus spread...</td>\n",
       "      <td>Coronavirus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2548</th>\n",
       "      <td>Explained: The ‘Bhilwara model’ of ‘ruthless c...</td>\n",
       "      <td>Coronavirus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2549</th>\n",
       "      <td>India to extend lockdown by 2 weeks  www reddi...</td>\n",
       "      <td>Coronavirus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2550</th>\n",
       "      <td>My name is Chang and I am not coronavirus  you...</td>\n",
       "      <td>Coronavirus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2551</th>\n",
       "      <td>Coronavirus | Schools, colleges may remain clo...</td>\n",
       "      <td>Coronavirus</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2552 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            title + url        flair\n",
       "0     Untouchability, even in quarantine. 'We have n...    Scheduled\n",
       "1     Delhi Govt Sources: Names of CM Arvind Kejriwa...    Scheduled\n",
       "2     Delhi: AP Singh, advocate of 2012 Delhi gang-r...    Scheduled\n",
       "3     Why the Supreme Court’s verdict on SC/ST quota...    Scheduled\n",
       "4     What about the entrance exams scheduled in May...    Scheduled\n",
       "...                                                 ...          ...\n",
       "2547  Coronavirus: The race to stop the virus spread...  Coronavirus\n",
       "2548  Explained: The ‘Bhilwara model’ of ‘ruthless c...  Coronavirus\n",
       "2549  India to extend lockdown by 2 weeks  www reddi...  Coronavirus\n",
       "2550  My name is Chang and I am not coronavirus  you...  Coronavirus\n",
       "2551  Coronavirus | Schools, colleges may remain clo...  Coronavirus\n",
       "\n",
       "[2552 rows x 2 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array(['mind', 'blowing', 'fact', 'manmohan', 'singh', 'www', 'reddit',\n",
      "       'com', 'r', 'india', 'comment', 'd9fh9n', 'mind', 'blowing',\n",
      "       'fact', 'manmohan', 'singh'], dtype='<U8'), 'Science/Technology')\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test, token_vec = get_data(df, 'title + url')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=2, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1 = MultinomialNB(alpha = 2)\n",
    "clf1.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.603448275862069"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=2.5, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2 = LogisticRegression(C = 2.5)\n",
    "clf2.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6755485893416928"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf3 = RandomForestClassifier()\n",
    "clf3.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6755485893416928"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf3.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=4, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf4 = SVC(C=4)\n",
    "clf4.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6551724137931034"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf4.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMMENTS+TITLE\n",
    "\n",
    "Now I am using both comments and titles as text to train my classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading both the datasets\n",
    "\n",
    "data1 = pd.read_csv('data3.csv')\n",
    "data2 = pd.read_csv('data4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## forming a new dataframe containing both title of the reddit post along with the text of top comments\n",
    "\n",
    "df = pd.DataFrame(data1['title'] + data2['body'], columns = ['title + comments'])\n",
    "df['flair'] = data1['flair']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing the bad symbols\n",
    "\n",
    "df[\"title + comments\"] = preprocess(df['title + comments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title + comments</th>\n",
       "      <th>flair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Untouchability, even in quarantine.  We have n...</td>\n",
       "      <td>Scheduled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Delhi Govt Sources  Names of CM Arvind Kejriwa...</td>\n",
       "      <td>Scheduled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Delhi  AP Singh, advocate of 2012 Delhi gang-r...</td>\n",
       "      <td>Scheduled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why the Supreme Court’s verdict on SC ST quota...</td>\n",
       "      <td>Scheduled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What about the entrance exams scheduled in May...</td>\n",
       "      <td>Scheduled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2544</th>\n",
       "      <td>Coronavirus lockdown  2 cops on duty injured i...</td>\n",
       "      <td>Coronavirus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2545</th>\n",
       "      <td>Why are patients fleeing India’s coronavirus i...</td>\n",
       "      <td>Coronavirus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2548</th>\n",
       "      <td>Explained  The ‘Bhilwara model’ of ‘ruthless c...</td>\n",
       "      <td>Coronavirus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2549</th>\n",
       "      <td>India to extend lockdown by 2 weeksCurrently s...</td>\n",
       "      <td>Coronavirus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2550</th>\n",
       "      <td>My name is Chang and I am not coronavirusYeah ...</td>\n",
       "      <td>Coronavirus</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2295 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       title + comments        flair\n",
       "0     Untouchability, even in quarantine.  We have n...    Scheduled\n",
       "1     Delhi Govt Sources  Names of CM Arvind Kejriwa...    Scheduled\n",
       "2     Delhi  AP Singh, advocate of 2012 Delhi gang-r...    Scheduled\n",
       "3     Why the Supreme Court’s verdict on SC ST quota...    Scheduled\n",
       "4     What about the entrance exams scheduled in May...    Scheduled\n",
       "...                                                 ...          ...\n",
       "2544  Coronavirus lockdown  2 cops on duty injured i...  Coronavirus\n",
       "2545  Why are patients fleeing India’s coronavirus i...  Coronavirus\n",
       "2548  Explained  The ‘Bhilwara model’ of ‘ruthless c...  Coronavirus\n",
       "2549  India to extend lockdown by 2 weeksCurrently s...  Coronavirus\n",
       "2550  My name is Chang and I am not coronavirusYeah ...  Coronavirus\n",
       "\n",
       "[2295 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Removing NaN values\n",
    "\n",
    "df.dropna(inplace = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array(['coronavirus', 'lockdown', 'pm', 'narendra', 'modi', 'address',\n",
      "       'nation', '10', 'tomorrow', 'amid', 'state', 'demand', 'extend',\n",
      "       'lockdown', 'till', 'april', '30it', 'time', 'ritual',\n",
      "       'smellwaiting', 'next', 'task.why', 'declare', 'whatever', 'want',\n",
      "       'declare', '1-2', 'day', 'before?', 'last', 'minute',\n",
      "       'declaration', 'cause', 'chaos.', 'hope', 'red-orange-green',\n",
      "       'thing.10am', '8pm', 'might', 'relax', 'condition', 'make', 'life',\n",
      "       'easy', 'non', 'impact', 'regions.ramayan', 'ke', 'baad', 'siddha',\n",
      "       'modiji', 'ke', 'darshanany', 'guess', 'would', 'new', 'task',\n",
      "       '?tn', 'already', 'extend', 'lockdown', 'till', 'april', '30',\n",
      "       'tweet', 'came.', 'tweet', 'http', 'twitter.com', 'pmoindia',\n",
      "       'status', '1249620775679610880?s', '21', 'bet', 'would', 'full',\n",
      "       'lockdown', 'extend', 'behalf', 'shift', 'decision', 'cm',\n",
      "       'respectively', 'state', 'bjp', 'shame', 'later', 'oppose',\n",
      "       'party', 'states.he', 'come', 'directly', 'tv,', 'first',\n",
      "       'announcement', 'give', 'time', 'everyone', 'glue', 'tv.', 'poora',\n",
      "       'tamasha', 'banaa', 'diya', 'hai.fir', 'shuru', 'tamasha.popcorn',\n",
      "       'timecan', 'fuck', 'press', 'releasei', 'urge', 'people', 'drink',\n",
      "       'gomutra', '15', '00', '15', 'th', 'april', 'support', 'people',\n",
      "       'work', 'keep', 'u', 'karuna', 'pm', 'sstock', 'gomutra',\n",
      "       'citizenswhy', 'press', 'conference', 'like', 'us?', 'everytime',\n",
      "       'video', 'releasedbhai', 'daru', 'kohl', 'dena.', 'delete'],\n",
      "      dtype='<U21'), 'Coronavirus')\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test, token_vec = get_data(df, 'title + comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(token_vec, open('transform1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=2, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1 = MultinomialNB(alpha = 2)\n",
    "clf1.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44947735191637633"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6533101045296167"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2 = LogisticRegression(tol = 0.00001, max_iter = 100)\n",
    "clf2.fit(x_train, y_train)\n",
    "clf2.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests Classifier (Chosen Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=150,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf3 = RandomForestClassifier(n_estimators = 150, criterion = 'gini')\n",
    "clf3.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(clf3, open('model1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7473867595818815"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf3.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf3.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "          AskIndia       0.68      0.75      0.71        61\n",
      "  Business/Finance       0.62      0.67      0.65        60\n",
      "       Coronavirus       0.75      0.84      0.79        45\n",
      "              Food       0.69      0.84      0.76        50\n",
      "     Non-Political       1.00      0.94      0.97        53\n",
      "       Photography       0.97      0.66      0.79        53\n",
      "    Policy/Economy       0.68      0.54      0.60        50\n",
      "          Politics       0.60      0.69      0.64        51\n",
      "         Scheduled       0.63      0.55      0.59        44\n",
      "Science/Technology       0.73      0.56      0.64        39\n",
      "            Sports       0.74      0.85      0.79        68\n",
      "\n",
      "          accuracy                           0.73       574\n",
      "         macro avg       0.74      0.72      0.72       574\n",
      "      weighted avg       0.74      0.73      0.73       574\n",
      "\n",
      "[[46  2  3  2  0  0  1  3  2  0  2]\n",
      " [ 4 40  1  0  0  0  6  3  1  2  3]\n",
      " [ 0  0 38  3  0  0  1  0  0  0  3]\n",
      " [ 1  0  1 42  0  0  0  3  2  0  1]\n",
      " [ 1  0  0  0 50  0  0  0  0  2  0]\n",
      " [ 5  2  0  4  0 35  0  2  3  0  2]\n",
      " [ 1  7  2  2  0  0 27  9  1  0  1]\n",
      " [ 2  0  1  6  0  1  2 35  3  0  1]\n",
      " [ 3  6  0  1  0  0  3  0 24  3  4]\n",
      " [ 3  5  2  1  0  0  0  2  1 22  3]\n",
      " [ 2  2  3  0  0  0  0  1  1  1 58]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.748739883600557"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_pred, average = \"weighted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf4 = SVC()\n",
    "clf4.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5958188153310104"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf4.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMMENT + TITLE + URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading both the datasets\n",
    "\n",
    "data1 = pd.read_csv('data3.csv')\n",
    "data2 = pd.read_csv('data4.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "## forming a new dataframe containing both title of the reddit post along with the text of top comments\n",
    "\n",
    "df = pd.DataFrame(data1['title'] + data1['url'] + data2['body'], columns = ['title + comments + url'])\n",
    "df['flair'] = data1['flair']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing the bad symbols\n",
    "\n",
    "df[\"title + comments + url\"] = preprocess(df['title + comments + url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title + comments + url</th>\n",
       "      <th>flair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Untouchability, even in quarantine   We have n...</td>\n",
       "      <td>Scheduled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Delhi Govt Sources: Names of CM Arvind Kejriwa...</td>\n",
       "      <td>Scheduled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Delhi: AP Singh, advocate of 2012 Delhi gang r...</td>\n",
       "      <td>Scheduled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why the Supreme Court’s verdict on SC ST quota...</td>\n",
       "      <td>Scheduled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What about the entrance exams scheduled in May...</td>\n",
       "      <td>Scheduled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2544</th>\n",
       "      <td>Coronavirus lockdown: 2 cops on duty injured i...</td>\n",
       "      <td>Coronavirus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2545</th>\n",
       "      <td>Why are patients fleeing India’s coronavirus i...</td>\n",
       "      <td>Coronavirus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2548</th>\n",
       "      <td>Explained: The ‘Bhilwara model’ of ‘ruthless c...</td>\n",
       "      <td>Coronavirus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2549</th>\n",
       "      <td>India to extend lockdown by 2 weeks  www reddi...</td>\n",
       "      <td>Coronavirus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2550</th>\n",
       "      <td>My name is Chang and I am not coronavirus  you...</td>\n",
       "      <td>Coronavirus</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2295 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 title + comments + url        flair\n",
       "0     Untouchability, even in quarantine   We have n...    Scheduled\n",
       "1     Delhi Govt Sources: Names of CM Arvind Kejriwa...    Scheduled\n",
       "2     Delhi: AP Singh, advocate of 2012 Delhi gang r...    Scheduled\n",
       "3     Why the Supreme Court’s verdict on SC ST quota...    Scheduled\n",
       "4     What about the entrance exams scheduled in May...    Scheduled\n",
       "...                                                 ...          ...\n",
       "2544  Coronavirus lockdown: 2 cops on duty injured i...  Coronavirus\n",
       "2545  Why are patients fleeing India’s coronavirus i...  Coronavirus\n",
       "2548  Explained: The ‘Bhilwara model’ of ‘ruthless c...  Coronavirus\n",
       "2549  India to extend lockdown by 2 weeks  www reddi...  Coronavirus\n",
       "2550  My name is Chang and I am not coronavirus  you...  Coronavirus\n",
       "\n",
       "[2295 rows x 2 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Removing NaN values\n",
    "\n",
    "df.dropna(inplace = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array(['coronavirus', 'lockdown:', 'pm', 'narendra', 'modi', 'address',\n",
      "       'nation', '10', 'tomorrow', 'amid', 'state', 'demand', 'extend',\n",
      "       'lockdown', 'till', 'april', '30', 'www', 'ndtv', 'com', 'india',\n",
      "       'news', 'coronavirus', 'lockdown', 'pm', 'narendra', 'modi',\n",
      "       'address', 'nation', '10', 'tomorrow', 'amid', 'state', 'demand',\n",
      "       'exte', '2210864it', 'time', 'ritual', 'smellwaiting', 'next',\n",
      "       'task', 'declare', 'whatever', 'want', 'declare', '1', '2', 'day',\n",
      "       'before?', 'last', 'minute', 'declaration', 'cause', 'chaos',\n",
      "       'hope', 'red', 'orange', 'green', 'thing', '10am', '8pm', 'might',\n",
      "       'relax', 'condition', 'make', 'life', 'easy', 'non', 'impact',\n",
      "       'region', 'ramayan', 'ke', 'baad', 'siddha', 'modiji', 'ke',\n",
      "       'darshanany', 'guess', 'would', 'new', 'task', '?tn', 'already',\n",
      "       'extend', 'lockdown', 'till', 'april', '30', 'tweet', 'come',\n",
      "       'tweet', 'twitter', 'com', 'pmoindia', 'status',\n",
      "       '1249620775679610880?s', '21', 'bet', 'would', 'full', 'lockdown',\n",
      "       'extend', 'behalf', 'shift', 'decision', 'cm', 'respectively',\n",
      "       'state', 'bjp', 'shame', 'later', 'oppose', 'party', 'state',\n",
      "       'come', 'directly', 'tv,', 'first', 'announcement', 'give', 'time',\n",
      "       'everyone', 'glue', 'tv', 'poora', 'tamasha', 'banaa', 'diya',\n",
      "       'hai', 'fir', 'shuru', 'tamasha', 'popcorn', 'timecan', 'fuck',\n",
      "       'press', 'releasei', 'urge', 'people', 'drink', 'gomutra', '15:00',\n",
      "       '15', 'th', 'april', 'support', 'people', 'work', 'keep', 'u',\n",
      "       'karuna', 'pm', 'sstock', 'gomutra', 'citizenswhy', 'press',\n",
      "       'conference', 'like', 'us?', 'everytime', 'video', 'releasedbhai',\n",
      "       'daru', 'kohl', 'dena', 'delete'], dtype='<U21'), 'Coronavirus')\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test, token_vec = get_data(df, 'title + comments + url')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=2, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1 = MultinomialNB(alpha = 2)\n",
    "clf1.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49477351916376305"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6968641114982579"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2 = LogisticRegression()\n",
    "clf2.fit(x_train, y_train)\n",
    "clf2.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf3 = RandomForestClassifier()\n",
    "clf3.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7038327526132404"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf3.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf3.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "          AskIndia       0.54      0.80      0.65        61\n",
      "  Business/Finance       0.66      0.55      0.60        60\n",
      "       Coronavirus       0.77      0.89      0.82        45\n",
      "              Food       0.58      0.88      0.70        50\n",
      "     Non-Political       0.98      0.89      0.93        53\n",
      "       Photography       0.93      0.70      0.80        53\n",
      "    Policy/Economy       0.68      0.46      0.55        50\n",
      "          Politics       0.59      0.67      0.62        51\n",
      "         Scheduled       0.56      0.57      0.56        44\n",
      "Science/Technology       0.72      0.54      0.62        39\n",
      "            Sports       0.98      0.75      0.85        68\n",
      "\n",
      "          accuracy                           0.70       574\n",
      "         macro avg       0.73      0.70      0.70       574\n",
      "      weighted avg       0.73      0.70      0.71       574\n",
      "\n",
      "[[49  2  1  3  0  0  0  2  4  0  0]\n",
      " [ 5 33  3  4  1  0  4  4  5  1  0]\n",
      " [ 0  0 40  5  0  0  0  0  0  0  0]\n",
      " [ 4  0  0 44  0  0  0  2  0  0  0]\n",
      " [ 2  0  0  1 47  0  0  1  0  2  0]\n",
      " [ 5  1  1  1  0 37  3  2  3  0  0]\n",
      " [ 5  5  2  3  0  0 23  6  4  2  0]\n",
      " [ 3  0  2  8  0  1  2 34  1  0  0]\n",
      " [ 6  3  0  4  0  0  2  2 25  2  0]\n",
      " [ 3  4  1  3  0  2  0  2  2 21  1]\n",
      " [ 8  2  2  0  0  0  0  3  1  1 51]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As we can see from the above results and the confusion matrix, the classifier works well for classification of flairs such as Sports and Non-Political.\n",
    "### However, in other categories, it gets confused due to a great amount of similarity between most of the flairs due to ongoing Covid19 crisis, because of which most of the other flairs have similar content. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf4 = SVC()\n",
    "clf4.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6411149825783972"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf4.score(x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
